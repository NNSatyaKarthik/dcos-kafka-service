# Kafka Framework

Management of Kafka clusters in DCOS

<!-- TOC START: generated by generate-md-toc.py, do not edit -->

- [Overview](#overview)
  - [Benefits](#benefits)
  - [Features](#features)
  - [Related Services](#related-services)
- [Quick Start](#quick-start)
- [Installation and Customization](#installation-and-customization)
  - [Default install configuration](#default-install-configuration)
  - [Custom install configuration](#custom-install-configuration)
  - [Uninstall](#uninstall)
  - [Changing configuration in flight](#changing-configuration-in-flight)
- [Configuration Options](#configuration-options)
  - [Framework Name](#framework-name)
  - [Broker Count](#broker-count)
  - [Enable Persistent Volumes](#enable-persistent-volumes)
  - [Configure Broker Placement Strategy](#configure-broker-placement-strategy)
- [Upgrading Software](#upgrading-software)
- [Connecting Clients](#connecting-clients)
- [Handling Errors](#handling-errors)
  - [Configuration Update Errors](#configuration-update-errors)
  - [Software Maintenance Errors](#software-maintenance-errors)
  - [Status Errors](#status-errors)
  - [Replacing a Permanently Failed Server](#replacing-a-permanently-failed-server)
- [Limitations](#limitations)
  - [Configurations](#configurations)
  - [Brokers](#brokers)
  - [Security](#security)
- [Cluster Maintenance APIs](#cluster-maintenance-apis)
  - [Connection Information](#connection-information)
  - [Broker Operations](#broker-operations)
    - [Add Broker](#add-broker)
    - [Remove Broker](#remove-broker)
    - [List All Brokers](#list-all-brokers)
    - [View Broker Details](#view-broker-details)
    - [Restart Single Broker](#restart-single-broker)
    - [Restart All Brokers](#restart-all-brokers)
  - [Topic Operations](#topic-operations)
    - [List Topics](#list-topics)
    - [Create Topic](#create-topic)
    - [View Topic Details](#view-topic-details)
    - [View Topic Offsets](#view-topic-offsets)
    - [Alter Topic Partition Count](#alter-topic-partition-count)
    - [Alter Topic Config Value](#alter-topic-config-value)
    - [Delete/Unset Topic Config Value](#deleteunset-topic-config-value)
    - [Run Producer Test on Topic](#run-producer-test-on-topic)
    - [Delete Topic](#delete-topic)
    - [List Under Replicated Partitions](#list-under-replicated-partitions)
    - [List Unavailable Partitions](#list-unavailable-partitions)
- [TODO [API for ACL changes](https://kafka.apache.org/documentation.html#security_authz_examples)? (kafka-acls.sh)](#todo-api-for-acl-changeshttpskafkaapacheorgdocumentationhtmlsecurity_authz_examples-kafka-aclssh)
- [Development](#development)

<!-- TOC END: generated by generate-md-toc.py, do not edit -->

## Overview

DCOS Kafka Service is an automated service framework that make it easy to deploy and manage Apache Kafka on Mesosphere DCOS. Apache Kafka is a distributed high throughput publish-subscribe messaging system with strong durability with ordering guarantees. For more information on Apache Kafka, see the Apache Kafka [documentation](http://kafka.apache.org/documentation.html). With DCOS Kafka Service, you get direct access to the Kafka API so that existing can interoperate. You can configure and install Kafka in less than minutes. Multiple Kafka clusters can be installed to DCOS and managed independently, enabling IT to offer Kafka as a managed service to the organization. Kafka clusters are highly available, e, fault tolerant, and highly durable. DCOS Kafka Service eliminates nearly all of the complexity traditionally associated with managing an Kafka cluster.

### Benefits

DCOS Kafka Service offers the following benefits of a semi-managed service:

- Easy installation
- Multiple Kafka clusters
- Elastic scaling of brokers
- Replication for high availability
- Kafka cluster and broker monitoring

### Features

DCOS Kafka Service provides the following features:

- Single command installation for rapid provisioning
- Multiple clusters for multiple tenancy with DCOS
- Runtime configuration and software update for high availability
- Storage volumes for enhanced data durability, known as Mesos Dynamic Reservations and Persistent Volumes
- Integration with syslog-compatible logging services for diagnostics and troubleshooting
- Integration with statsd-compatible metrics services for capacity and performance monitoring


### Related Services
- [DCOS Spark Service](https://docs.mesosphere.com/manage-service/spark)


## Quick Start

- Step 1. Install [dcos-cli](https://github.com/mesosphere/dcos-cli).

- Step 2. Install a Kafka cluster.

``` bash
$ dcos package install kafka
```

- Step 3. Create a new topic.

``` bash
$ curl -vX POST $DCOS_URI/service/kafka0/topics?name=topic0&partitions=3&replication=3 -pbH
```

- Step 4. Read and write data to a topic.

**TODO** a couple one-liners using eg `bin/kafka-console-[producer|consumer].sh` to send/get data. see also [Connecting clients](#connecting-clients)

- Step 5. Mark a topic for deletion.

``` bash
$ curl -vX DELETE $DCOS_URI/service/kafka0/v1/topics/topic0 -pbH
```

- Step 6. Uninstall the cluster.

``` bash
$ dcos package uninstall --app-id=kafka0 kafka
```

## Installation and Customization

### Default install configuration

To start a basic test cluster with three brokers, run the following command with dcos-cli:

``` bash
$ dcos package install kafka
```

By default, this will create a new Kafka cluster named `kafka0`. Two clusters cannot share the same name, so installing additional clusters beyond the default cluster would require [customizing the `framework-name` at install time](#custom-install-configuration) for each additional instance.

Additional customization would be needed before this cluster would be suitable for production use, but it should be plenty for testing/development as-is. Running clusters may be [re-configured in-place using Marathon](#changing-configuration-in-flight).

### Custom install configuration

The defaults may be customized by creating a JSON file, then passing it to `dcos package install` using the `--options parameter`. For example:

Sample JSON options file named `sample-kafka.json`:
``` json
{
  "kafka": {
    "broker-count": 10,
    "framework-name": "sample-kafka",
    "pv": true,
    "placement-strategy": "NODE"
  }
}
```

Creating a cluster using `sample-kafka.json`:
``` bash
$ dcos package install --options=sample-kafka.json kafka
```

See [Configuration Options](#configuration-options) for a list of available fields which can be customized via an options JSON file when the Kafka cluster is created.

### Uninstall

Uninstalling a cluster is also straightforward. Replace `kafka0` with the name of the kafka instance to be uninstalled.

``` bash
$ dcos package uninstall --app-id=kafka0 kafka
```

**TODO** are extra steps needed for dropping any persistent volumes?

**TODO** is the instance cleared from zookeeper? if not, what extra steps are needed to do that?

### Changing configuration in flight

Once the cluster is already up and running, it may be customized in-place. The Kafka Scheduler will be running as a Marathon process, and can be reconfigured by changing values within Marathon.

1. View your Marathon dashboard at `http://$DCOS_URI/marathon`
2. In the list of `Applications`, click the name of the Kafka framework to be updated.
3. Within the Kafka instance details view, click the `Configuration` tab, then click the `Edit` button.
4. In the dialog that appears, expand the `Environment Variables` section and update any field(s) to their desired value(s). For example, to [increase the number of Brokers](#broker-count), edit the value for `BROKER_COUNT`. Do not edit the value for `FRAMEWORK_NAME`.
5. Click `Change and deploy configuration` to apply any changes and cleanly reload the Kafka Framework scheduler. The Kafka cluster itself will persist across the change.

See [Configuration Options](#configuration-options) for a list of available fields which can be customized via Marathon while the Kafka cluster is running.

## Configuration Options

The following describes commonly used features of the Kafka framework and how to configure them. View the [default `config.json` in DCOS Universe](https://github.com/mesosphere/universe/tree/version-1.x/repo/packages/K/kafka) to see an enumeration of all possible options.

### Framework Name

The name of this Kafka instance in DCOS. This is the only option that cannot be changed once the Kafka cluster is started; it can only be configured via the `dcos-cli --options` flag when first creating the Kafka instance.

- **In dcos-cli options.json**: `framework-name` = string (default: `kafka0`)
- **In Marathon**: The Framework Name cannot be changed after the cluster has started.

### Broker Count

Configure the number of brokers running in a given Kafka cluster. The default count at installation is three brokers.

- **In dcos-cli options.json**: `broker-count` = integer (default: `3`)
- **In Marathon**: `BROKER_COUNT` = integer

### Enable Persistent Volumes

By default Kafka Brokers will use the sandbox available to Mesos Tasks for storing data. This storage goes away on Task failure. So if a Broker crashes the data on it is lost forever. This is fine for dev environments. In production environments Kafka should be deployed with the following option enabled:

- **In dcos-cli options.json**: `pv` = boolean (default: `false`)
- **In Marathon**: `BROKER_PV` = `TRUE` or `FALSE`

### Configure Broker Placement Strategy

`ANY` will allow brokers to be placed on any node with sufficient resources, while `NODE` will ensure that all brokers within a given Kafka cluster are never colocated on the same node. In the future, this may also prevent brokers across multiple Kafka clusters from colocating, but this isn't yet the case.

- **In dcos-cli options.json**: `placement-strategy` = `ANY` or `NODE` (default: `ANY`)
- **In Marathon**: `PLACEMENT_STRATEGY` = `ANY` or `NODE`

## Upgrading Software

**TODO** guide for updating kafka and/or framework

## Connecting Clients

**TODO** step by step for talking to a DCOS-hosted Kafka instance using `bin/kafka-console-producer.sh` and `bin/kafka-console-consumer.sh` as an example

## Handling Errors

### Configuration Update Errors

**TODO** How to handle configuration input validation errors

**TODO** How to handle errors when applying configuration to a running service

### Software Maintenance Errors

**TODO** How to handle errors when updating software

### Status Errors

**TODO** how to handle status = error

**TODO** how to handle status = warning

### Replacing a Permanently Failed Server

**TODO** How to tell the scheduler a specific Mesos Agent is dead and never coming back

## Limitations

### Configurations

**TODO** Settings that can only be set at install time

**TODO** Pitfalls of managing configurations outside of the framework

### Brokers

**TODO** Max count (if any)

### Security

**TODO** describe how someone could configure Kafka 0.9's beta security features (or specify that they're not supported?): data encryption, client authentication over SSL/SASL, broker authentication with ZK...

## Cluster Maintenance APIs

For ongoing maintenance of the Kafka cluster itself, the Kafka Framework exposes an HTTP API whose structure is designed to roughly match the tools provided by the Kafka distribution, such as `bin/kafka-topics.sh`.

The examples provided here use the `curl` utility to query the Kafka service over HTTP. These examples assume a DCOS host of `$DCOS_URI` and a Kafka framework named `kafka0` (the default initial framework name). Replace these with appropriate values as needed.

### Connection Information

Kafka comes with many useful tools of its own. They often require either Zookeeper connection information, or the list of Broker endpoints. This information can be retrieved in an easily consumable formation from the `/connection` endpoint as below.

``` bash
$ curl -v $DCOS_URI/service/kafka0/connection -pbH
GET /service/kafka0/connection HTTP/1.1
[...]

{
    "brokers": [
        "10.0.0.1:9092",
        "10.0.0.2:9093",
        "10.0.0.3:9094"
    ],
    "zookeeper": "master.mesos:2181/kafka0"
}
```

### Broker Operations

#### Add Broker

Increase the `BROKER_COUNT` value via Marathon. New brokers should start automatically.

#### Remove Broker

Broker removal is currently a manual process.

**TODO**: Specify step-by-step for manually removing broker(s)

#### List All Brokers

``` bash
$ curl -v $DCOS_URI/service/kafka0/brokers -pbH
GET /service/kafka0/brokers HTTP/1.1
[...]

[
    "0",
    "1",
    "2"
]
```
#### View Broker Details

``` bash
$ curl -v $DCOS_URI/service/kafka0/brokers/0 -pbH
GET /service/kafka0/brokers/0 HTTP/1.1
[...]

{
    "endpoints": [
        "PLAINTEXT://worker12398:9092"
    ],
    "host": "worker12398",
    "jmx_port": -1,
    "port": 9092,
    "timestamp": "1453854226816",
    "version": 2
}
```

#### Restart Single Broker

``` bash
$ curl -vX PUT $DCOS_URI/service/kafka0/brokers/0 -pbH
PUT /service/kafka0/brokers/0 HTTP/1.1
[...]

[
    "broker-0__9c426c50-1087-475c-aa36-cd00d24ccebb"
]
```

#### Restart All Brokers

``` bash
$ curl -vX PUT $DCOS_URI/service/kafka0/brokers -pbH
PUT /service/kafka0/brokers HTTP/1.1
[...]

[
    "broker-1__759c9fc2-3890-4921-8db8-c87532b1a033",
    "broker-2__0b444104-e210-4b78-8d19-f8938b8761fd",
    "broker-0__9c426c50-1087-475c-aa36-cd00d24ccebb"
]
```

### Topic Operations

These operations mirror what's available using `bin/kafka-topics.sh`.

#### List Topics

``` bash
$ curl -v $DCOS_URI/service/kafka0/topics -pbH
GET /service/kafka0/topics HTTP/1.1
[...]

[
    "topic1",
    "topic0"
]
```

#### Create Topic

``` bash
$ curl -vX POST $DCOS_URI/service/kafka0/topics?name=topic1&partitions=3&replication=3 -pbH
POST /service/kafka0/topics?replication=3&name=topic1&partitions=3 HTTP/1.1
[...]

{
    "exit_code": 0,
    "stderr": "",
    "stdout": "Created topic \"topic1\".\n"
}
```

#### View Topic Details

``` bash
$ curl -v $DCOS_URI/service/kafka0/topics/topic1 -pbH
GET /service/kafka0/topics/topic1 HTTP/1.1
[...]

{
    "partitions": [
        {
            "0": {
                "controller_epoch": 1,
                "isr": [
                    0,
                    1,
                    2
                ],
                "leader": 0,
                "leader_epoch": 0,
                "version": 1
            }
        },
        {
            "1": {
                "controller_epoch": 1,
                "isr": [
                    1,
                    2,
                    0
                ],
                "leader": 1,
                "leader_epoch": 0,
                "version": 1
            }
        },
        {
            "2": {
                "controller_epoch": 1,
                "isr": [
                    2,
                    0,
                    1
                ],
                "leader": 2,
                "leader_epoch": 0,
                "version": 1
            }
        }
    ]
}
```

#### View Topic Offsets

``` bash
$ curl -v $DCOS_URI/service/kafka0/topics/topic1/offsets -pbH
GET /service/kafka0/topics/topic1/offsets HTTP/1.1
[...]

[
    {
        "2": "0"
    },
    {
        "1": "0"
    },
    {
        "0": "0"
    }
]
```

#### Alter Topic Partition Count

``` bash
$ curl -vX PUT $DCOS_URI/service/kafka0/topics/topic1?operation=partitions&partitions=2 -pbH
PUT /service/kafka0/topics/topic1?operation=partitions&partitions=2 HTTP/1.1
[...]

{
    "exit_code": 0,
    "stderr": "",
    "stdout": "WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected\nAdding partitions succeeded!\n"
}
```

#### Alter Topic Config Value

``` bash
$ curl -vX PUT $DCOS_URI/service/kafka0/topics/topic1?operation=config&key=foo&value=bar
PUT /service/kafka0/topics/topic1?operation=config&key=foo&value=bar HTTP/1.1
[...]

{
    "exit_code": 1,
    "stderr": "[...] Unknown configuration \"foo\". [...]",
    "stdout": "[...] Unknown configuration \"foo\". [...]"
}
```

#### Delete/Unset Topic Config Value

``` bash
$ curl -vX PUT $DCOS_URI/service/kafka0/topics/topic1?operation=deleteConfig&key=foo
```

#### Run Producer Test on Topic

``` bash
$ curl -vX PUT $DCOS_URI/service/kafka0/topics/topic1?operation=producer-test&messages=10 -pbH
PUT /service/kafka0/topics/topic1?operation=producer-test&messages=10 HTTP/1.1
[...]

{
    "exit_code": 0,
    "stderr": "",
    "stdout": "10 records sent, 70.422535 records/sec (0.07 MB/sec), 24.20 ms avg latency, 133.00 ms max latency, 13 ms 50th, 133 ms 95th, 133 ms 99th, 133 ms 99.9th.\n"
}
```

#### Delete Topic

``` bash
$ curl -X DELETE $DCOS_URI/service/kafka0/topics/topic1 -pbH
DELETE /service/kafka0/topics/topic1?operation=delete HTTP/1.1
[...]

{
    "exit_code": 0,
    "stderr": "",
    "stdout": "Topic topic1 is marked for deletion.\nNote: This will have no impact if delete.topic.enable is not set to true.\n"
}
```

#### List Under Replicated Partitions

``` bash
$ curl -v $DCOS_URI/service/kafka0/topics/under_replicated_partitions -pbH
GET /service/kafka0/topics/under_replicated_partitions HTTP/1.1
[...]

{
    "exit_code": 0,
    "stderr": "",
    "stdout": ""
}
```

#### List Unavailable Partitions

``` bash
$ curl -v $DCOS_URI/service/kafka0/topics/unavailable_partitions -pbH
GET /service/kafka0/topics/unavailable_partitions HTTP/1.1
[...]

{
    "exit_code": 0,
    "stderr": "",
    "stdout": ""
}
```

## TODO [API for ACL changes](https://kafka.apache.org/documentation.html#security_authz_examples)? (kafka-acls.sh)

## Development

See [CONTRIBUTING.md](CONTRIBUTING.md) for the development guide.

